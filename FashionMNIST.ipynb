{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d18055d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5171995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424a723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182c873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6321cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "'cuda:0' if torch.cuda.is_available() else 'cpu' \n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8572a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28f41ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 1\n",
    "lr = 1e-4\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "705ec618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "image_size = 28\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8458ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    " #   transforms.ToPILImage(),   # 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2740648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间\n",
    "from torchvision import datasets\n",
    "root_dir = '/home/yunwanx/git/ds/fashionMNIST'\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = root_dir,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = data_transform\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=root_dir, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=data_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a090b5a5",
   "metadata": {},
   "source": [
    "#读取方式二， 这一步不运行\n",
    "class FMDataset(Dataset):\n",
    "    def __init__(self,df,transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = df.iloc[:,1:].values.astype(np.uint8)\n",
    "        slef.labels = df.iloc[:,0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx].reshape(28,28,1)\n",
    "        label = int(self.labels[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image/255.,dtype=torch.float)\n",
    "        label = torch.tensor(label,dtype=torch.long)\n",
    "        return image, label\n",
    "    \n",
    "    train_df = pd.read_csv('/home/yunwanx/git/ds/fashionMNIST/FashionMNIST/train.csv')\n",
    "    test_df = pd.read_csv('/home/yunwanx/git/ds/fashionMNIST/FashionMNIST/test.csv')\n",
    "    train_data = FMDataset(train_df, data_transform)\n",
    "    test_data = FMDataset(test_df, data_transform)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "782ecafe",
   "metadata": {},
   "source": [
    "train_data和test_data都是Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b39a4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/yunwanx/git/ds/fashionMNIST\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=28, interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8aca84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ecaf6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f581c2899d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape, label.shape)\n",
    "plt.imshow(image[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5635cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = self.fc(x)\n",
    "        # x = nn.functional.normalize(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "#model = model.cuda() #gpu\n",
    "# model = nn.DataParallel(model).cuda() # 多卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f4febcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fee90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6594110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, label in train_loader:\n",
    "        #data, label = data.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "525ff990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    gt_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "           # data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            gt_labels.append(label.cpu().data.numpy())\n",
    "            pred_labels.append(preds.cpu().data.numpy())\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "    val_loss = val_loss/len(test_loader.dataset)\n",
    "    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)\n",
    "    acc = np.sum(gt_labels==pred_labels)/len(pred_labels)\n",
    "    print('Epoch: {} \\tValidation Loss: {:.6f}, Accuracy: {:6f}'.format(epoch, val_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35bced6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.658910\n",
      "Epoch: 1 \tValidation Loss: 0.480774, Accuracy: 0.824100\n",
      "Epoch: 2 \tTraining Loss: 0.421687\n",
      "Epoch: 2 \tValidation Loss: 0.369700, Accuracy: 0.868200\n",
      "Epoch: 3 \tTraining Loss: 0.364189\n",
      "Epoch: 3 \tValidation Loss: 0.325282, Accuracy: 0.883800\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    val(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23e2e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./FahionModel.pkl\"\n",
    "torch.save(model, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('miniconda3.7': virtualenv)",
   "language": "python",
   "name": "python37664bitminiconda37virtualenv16c5a718c8d74f97be1cf5451fe5f024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
